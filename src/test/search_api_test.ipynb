{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_community.tools.google_scholar import GoogleScholarQueryRun\n",
    "from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper\n",
    "\n",
    "dotenv_file = dotenv.find_dotenv(\"/home/sangbin_yun/dev/llm_agent/.env\")\n",
    "dotenv.load_dotenv(dotenv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SerpAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serp api\n",
    "tool = GoogleScholarQueryRun(api_wrapper=GoogleScholarAPIWrapper())\n",
    "test = tool.run('EEG AND \"conflict\" AND \"PLV\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scholary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarly import scholarly, ProxyGenerator\n",
    "\n",
    "# Set up a ProxyGenerator object to use free proxies\n",
    "# This needs to be done only once per session\n",
    "# pg = ProxyGenerator()\n",
    "# pg.FreeProxies()\n",
    "# scholarly.use_proxy(pg)\n",
    "\n",
    "search_query = scholarly.search_pubs('EEG AND \"conflict\" AND \"PLV\"')\n",
    "print(search_query.total_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_eprint = 0\n",
    "not_total_eprint = 0\n",
    "for i in range(200):\n",
    "    test = next(search_query)\n",
    "    # print(f\"===================={i}====================\")\n",
    "    # print(test[\"bib\"][\"abstract\"])\n",
    "    # print(test[\"pub_url\"])\n",
    "    try:\n",
    "        # print(test[\"eprint_url\"])\n",
    "        total_eprint += 1\n",
    "    except:\n",
    "        # print(\"No eprint_url\")\n",
    "        not_total_eprint += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Agent settnig\n",
    "# https://useragentstring.com/pages/Chrome/\n",
    "\n",
    "# whatismybrowser API as a user agent\n",
    "# https://developers.whatismybrowser.com/api/docs/v2/integration-guide/#software-version-numbers\n",
    "headers = {\n",
    "    \"X-API-KEY\": os.getenv(\"X-API-KEY\"),  # Your API Key goes here.\n",
    "}\n",
    "\n",
    "# Fake user agent setting\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "print(userAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxy\n",
    "from fp.fp import FreeProxy\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "ua = UserAgent()\n",
    "\n",
    "def get_http_proxy(https = False):\n",
    "    while True:\n",
    "        try:\n",
    "            proxy = FreeProxy(country_id = ['KR'], rand = True, https = https).get()\n",
    "            return proxy1\n",
    "        except:\n",
    "            None\n",
    "\n",
    "proxy = get_http_proxy(False)\n",
    "proxy2 = get_http_proxy(True)\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": ua.random\n",
    "}\n",
    "proxies = {\n",
    "    'http': proxy,\n",
    "    'https': proxy2,\n",
    "}\n",
    "params = {\n",
    "    'q': 'EEG ang plv',  # search query\n",
    "    'hl': 'en'       # language of the search\n",
    "}\n",
    "\n",
    "html = requests.get(\n",
    "    'https://scholar.google.com/scholar', \n",
    "    headers = headers,\n",
    "    proxies = proxies, \n",
    "    params = params\n",
    ").text\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, lxml, os, json\n",
    "\n",
    "def get_http_proxy(https = False):\n",
    "    return FreeProxy(country_id = ['KR'], rand = True, https = https).get()\n",
    "\n",
    "def scrape_one_google_scholar_page():\n",
    "    # headers = {\n",
    "    #     'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36'\n",
    "    # }\n",
    "    proxies = {\n",
    "        'http': get_http_proxy(False),\n",
    "        'https': get_http_proxy(True),\n",
    "    }\n",
    "    params = {\n",
    "        'q': 'samsung',  # search query\n",
    "        'hl': 'en'       # language of the search\n",
    "    }\n",
    "\n",
    "    html = requests.get('https://scholar.google.com/scholar', proxies = proxies, params = params).text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # JSON data will be collected here\n",
    "    data = []\n",
    "\n",
    "    # Container where all needed data is located\n",
    "    for result in soup.select('.gs_r.gs_or.gs_scl'):\n",
    "        title = result.select_one('.gs_rt').text\n",
    "        title_link = result.select_one('.gs_rt a')['href']\n",
    "        publication_info = result.select_one('.gs_a').text\n",
    "        snippet = result.select_one('.gs_rs').text\n",
    "        cited_by = result.select_one('#gs_res_ccl_mid .gs_nph+ a')['href']\n",
    "        try:\n",
    "            pdf_link = result.select_one('.gs_or_ggsm a:nth-child(1)')['href']\n",
    "        except: \n",
    "            pdf_link = None\n",
    "\n",
    "        data.append({\n",
    "            'title': title,\n",
    "            'title_link': title_link,\n",
    "            'publication_info': publication_info,\n",
    "            'snippet': snippet,\n",
    "            'cited_by': f'https://scholar.google.com{cited_by}',\n",
    "            'related_articles': f'https://scholar.google.com{related_articles}',\n",
    "            'all_article_versions': f'https://scholar.google.com{all_article_versions}',\n",
    "            \"pdf_link\": pdf_link\n",
    "        })\n",
    "\n",
    "    print(json.dumps(data, indent = 2, ensure_ascii = False))\n",
    "\n",
    "scrape_one_google_scholar_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "DATA_PATH = \"/mnt/c/Users/beene/Downloads\"\n",
    "\n",
    "loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "documents = loader.load()\n",
    "print(f\"Processed {len(documents)} pdf files\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "texts=text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(documents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent-Xw65Epl3-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
