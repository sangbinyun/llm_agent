{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools import tool\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import requests, os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "# Tool 1 : Save the news articles in a database\n",
    "class SearchNewsDB:\n",
    "    @tool(\"News DB Tool\")\n",
    "    def news(query: str):\n",
    "        \"\"\"Fetch news articles and process their contents.\"\"\"\n",
    "        API_KEY = os.getenv('NEWSAPI_KEY')  # Fetch API key from environment variable\n",
    "        base_url = \"https://newsapi.org/v2/everything\"\n",
    "        \n",
    "        params = {\n",
    "            'q': query,\n",
    "            'sortBy': 'publishedAt',\n",
    "            'apiKey': API_KEY,\n",
    "            'language': 'en',\n",
    "            'pageSize': 5,\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            return \"Failed to retrieve news.\"\n",
    "        \n",
    "        articles = response.json().get('articles', [])\n",
    "        all_splits = []\n",
    "        for article in articles:\n",
    "            # Assuming WebBaseLoader can handle a list of URLs\n",
    "            loader = WebBaseLoader(article['url'])\n",
    "            docs = loader.load()\n",
    "\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            splits = text_splitter.split_documents(docs)\n",
    "            all_splits.extend(splits)  # Accumulate splits from all articles\n",
    "\n",
    "        # Index the accumulated content splits if there are any\n",
    "        if all_splits:\n",
    "            vectorstore = Chroma.from_documents(all_splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n",
    "            retriever = vectorstore.similarity_search(query)\n",
    "            return retriever\n",
    "        else:\n",
    "            return \"No content available for processing.\"\n",
    "\n",
    "# Tool 2 : Get the news articles from the database\n",
    "class GetNews:\n",
    "    @tool(\"Get News Tool\")\n",
    "    def news(query: str) -> str:\n",
    "        \"\"\"Search Chroma DB for relevant news information based on a query.\"\"\"\n",
    "        vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
    "        retriever = vectorstore.similarity_search(query)\n",
    "        return retriever\n",
    "\n",
    "# Tool 3 : Search for news articles on the web\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# 2. Creating Agents\n",
    "news_search_agent = Agent(\n",
    "    role='News Seacher',\n",
    "    goal='Generate key points for each news article from the latest news',\n",
    "    backstory='Expert in analysing and generating key points from news content for quick updates.',\n",
    "    tools=[SearchNewsDB().news],\n",
    "    allow_delegation=True,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "writer_agent = Agent(\n",
    "    role='Writer',\n",
    "    goal='Identify all the topics received. Use the Get News Tool to verify the each topic to search. Use the Search tool for detailed exploration of each topic. Summarise the retrieved information in depth for every topic.',\n",
    "    backstory='Expert in crafting engaging narratives from complex information.',\n",
    "    tools=[GetNews().news, search_tool],\n",
    "    allow_delegation=True,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# 3. Creating Tasks\n",
    "news_search_task = Task(\n",
    "    description='Search for AI 2024 and create key points for each news.',\n",
    "    agent=news_search_agent,\n",
    "    tools=[SearchNewsDB().news]\n",
    ")\n",
    "\n",
    "writer_task = Task(\n",
    "    description=\"\"\"\n",
    "    Go step by step.\n",
    "    Step 1: Identify all the topics received.\n",
    "    Step 2: Use the Get News Tool to verify the each topic by going through one by one.\n",
    "    Step 3: Use the Search tool to search for information on each topic one by one. \n",
    "    Step 4: Go through every topic and write an in-depth summary of the information retrieved.\n",
    "    Don't skip any topic.\n",
    "    \"\"\",\n",
    "    agent=writer_agent,\n",
    "    context=[news_search_task],\n",
    "    tools=[GetNews().news, search_tool]\n",
    ")\n",
    "\n",
    "# 4. Creating Crew\n",
    "news_crew = Crew(\n",
    "    agents=[news_search_agent, writer_agent],\n",
    "    tasks=[news_search_task, writer_task],\n",
    "    process=Process.sequential, \n",
    "    manager_llm=llm\n",
    ")\n",
    "\n",
    "# Execute the crew to see RAG in action\n",
    "result = news_crew.kickoff()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
