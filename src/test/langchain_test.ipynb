{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from scholarly import scholarly\n",
    "from langchain_community.tools.google_scholar import GoogleScholarQueryRun\n",
    "from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nemo Guardrails\n",
    "from nemoguardrails import RailsConfig\n",
    "from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails\n",
    "\n",
    "# ... initialize `some_chain`\n",
    "\n",
    "config = RailsConfig.from_path(\"path/to/config\")\n",
    "\n",
    "# Using LCEL, you first create a RunnableRails instance, and \"apply\" it using the \"|\" operator\n",
    "guardrails = RunnableRails(config)\n",
    "chain_with_guardrails = guardrails | some_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "\n",
    "# 1. Create prompt template\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('user', '{text}')\n",
    "])\n",
    "\n",
    "# 2. Create model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 3. Create parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4. Create chain\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "# 4. App definition\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 5. Adding chain route\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/chain\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 pdf files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 45.9M/45.9M [00:04<00:00, 10.3MiB/s]\n",
      "Verifying: 100%|██████████| 45.9M/45.9M [00:00<00:00, 809MiB/s]\n",
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.12: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.12: cannot open shared object file: No such file or directory\n",
      "/home/sangbin_yun/.cache/pypoetry/virtualenvs/llm-agent-Xw65Epl3-py3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader, BSHTMLLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings  \n",
    "\n",
    "import os\n",
    "\n",
    "DATA_PATH=\"/mnt/c/Users/beene/Downloads\"\n",
    "DB_PATH = \"vectorstores/db/\"\n",
    "\n",
    "def create_vector_db():\n",
    "    loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    documents = loader.load()\n",
    "    print(f\"Processed {len(documents)} pdf files\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    texts=text_splitter.split_documents(documents)\n",
    "    vectorstore = Chroma.from_documents(documents=texts, embedding=GPT4AllEmbeddings(),persist_directory=DB_PATH)      \n",
    "    vectorstore.persist()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    create_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chainlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CallbackManager\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_stdout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingStdOutCallbackHandler\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchainlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcl\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA, RetrievalQAWithSourcesChain\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Set up RetrievelQA model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chainlit'"
     ]
    }
   ],
   "source": [
    "#import required dependencies\n",
    "from langchain import hub\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import chainlit as cl\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "# Set up RetrievelQA model\n",
    "QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt-mistral\")\n",
    "\n",
    "#load the LLM\n",
    "def load_llm():\n",
    "    llm = Ollama(\n",
    "        model=\"llama3\",\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        verbose=True,\n",
    "        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "def retrieval_qa_chain(llm,vectorstore):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "def qa_bot(): \n",
    "    llm=load_llm() \n",
    "    DB_PATH = \"vectorstores/db/\"\n",
    "    vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=GPT4AllEmbeddings())\n",
    "\n",
    "    qa = retrieval_qa_chain(llm,vectorstore)\n",
    "    return qa \n",
    "\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    chain=qa_bot()\n",
    "    msg=cl.Message(content=\"Firing up the research info bot...\")\n",
    "    await msg.send()\n",
    "    msg.content= \"Hi, welcome to research info bot. What is your query?\"\n",
    "    await msg.update()\n",
    "    cl.user_session.set(\"chain\",chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message):\n",
    "    chain=cl.user_session.get(\"chain\")\n",
    "    cb = cl.AsyncLangchainCallbackHandler(\n",
    "    stream_final_answer=True,\n",
    "    answer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\n",
    "    )\n",
    "    cb.answer_reached=True\n",
    "    # res=await chain.acall(message, callbacks=[cb])\n",
    "    res=await chain.acall(message.content, callbacks=[cb])\n",
    "    print(f\"response: {res}\")\n",
    "    answer=res[\"result\"]\n",
    "    answer=answer.replace(\".\",\".\\n\")\n",
    "    sources=res[\"source_documents\"]\n",
    "\n",
    "    if sources:\n",
    "        answer+=f\"\\nSources: \"+str(str(sources))\n",
    "    else:\n",
    "        answer+=f\"\\nNo Sources found\"\n",
    "\n",
    "    await cl.Message(content=answer).send() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent-Xw65Epl3-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
