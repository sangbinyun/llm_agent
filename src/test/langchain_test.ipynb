{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from scholarly import scholarly\n",
    "from langchain_community.tools.google_scholar import GoogleScholarQueryRun\n",
    "from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langserve import add_routes\n",
    "\n",
    "# 1. Create prompt template\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('user', '{text}')\n",
    "])\n",
    "\n",
    "# 2. Create model\n",
    "# model = ChatOpenAI()\n",
    "\n",
    "# 3. Create parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4. Create chain\n",
    "chain = prompt_template | llm | parser\n",
    "\n",
    "\n",
    "# 4. App definition\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 5. Adding chain route\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/chain\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector db #1\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader, BSHTMLLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings  \n",
    "\n",
    "# DATA_PATH=\"/mnt/c/Users/beene/Downloads/papers/\"\n",
    "DATA_PATH=\"/mnt/c/Users/beene/Downloads/tests/\"\n",
    "DB_PATH = \"./vectorstores/db/\"\n",
    "\n",
    "#load the LLM\n",
    "def load_llm():\n",
    "    llm = Ollama(\n",
    "        model=\"llama3\",\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        verbose=True,\n",
    "        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def create_vector_db():\n",
    "    loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    documents = loader.load()\n",
    "    print(f\"Processed {len(documents)} pdf files\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    texts=text_splitter.split_documents(documents)\n",
    "    vectorstore = Chroma.from_documents(documents=texts, embedding=OllamaEmbeddings(), persist_directory=DB_PATH)\n",
    "\n",
    "def load_vector_db():\n",
    "    return Chroma(persist_directory=DB_PATH, embedding_function=OllamaEmbeddings())\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    llm = load_llm()\n",
    "    # create_vector_db()\n",
    "    vectorstore = load_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "from langserve import add_routes\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"Spin up a simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    llm,\n",
    "    path=\"/llm\",\n",
    ")\n",
    "# add_routes(\n",
    "#     app,\n",
    "#     ChatAnthropic(model=\"claude-3-haiku-20240307\"),\n",
    "#     path=\"/anthropic\",\n",
    "# )\n",
    "\n",
    "async def main():\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector db #2\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings  \n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.create_collection(\"default\")\n",
    "collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "# langchain_chroma = Chroma(\n",
    "#     client=persistent_client,\n",
    "#     collection_name=\"default\",\n",
    "#     embedding_function=OllamaEmbeddings(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chroma.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = chromadb.PersistentClient(\n",
    "    path = \"./vectorstores/db/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = Chroma(\n",
    "    client = test,\n",
    "    collection_name = \"default\",\n",
    "    embedding_function = GPT4AllEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual compression retriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "\n",
    "compressor = FlashrankRerank(top_n = 1)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")\n",
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multiquery_retriever = MultiQueryRetriever.from_llm( \n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    ")\n",
    "compression_retriever.invoke(\"What did the president say about Ketanji Jackson Brown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required dependencies\n",
    "from langchain import hub\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import chainlit as cl\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "# Set up RetrievelQA model\n",
    "QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt-mistral\")\n",
    "\n",
    "\n",
    "def retrieval_qa_chain(llm,vectorstore):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "def qa_bot(): \n",
    "    llm=load_llm() \n",
    "    vectorstore = load_vector_db()\n",
    "\n",
    "    qa = retrieval_qa_chain(llm,vectorstore)\n",
    "    return qa \n",
    "\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    chain=qa_bot()\n",
    "    msg=cl.Message(content=\"Firing up the research info bot...\")\n",
    "    await msg.send()\n",
    "    msg.content= \"Hi, welcome to research info bot. What is your query?\"\n",
    "    await msg.update()\n",
    "    cl.user_session.set(\"chain\",chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message):\n",
    "    chain=cl.user_session.get(\"chain\")\n",
    "    cb = cl.AsyncLangchainCallbackHandler(\n",
    "    stream_final_answer=True,\n",
    "    answer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\n",
    "    )\n",
    "    cb.answer_reached=True\n",
    "    # res=await chain.acall(message, callbacks=[cb])\n",
    "    res=await chain.acall(message.content, callbacks=[cb])\n",
    "    print(f\"response: {res}\")\n",
    "    answer=res[\"result\"]\n",
    "    answer=answer.replace(\".\",\".\\n\")\n",
    "    sources=res[\"source_documents\"]\n",
    "\n",
    "    if sources:\n",
    "        answer+=f\"\\nSources: \"+str(str(sources))\n",
    "    else:\n",
    "        answer+=f\"\\nNo Sources found\"\n",
    "\n",
    "    await cl.Message(content=answer).send() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading client ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# to ignore the deprecation warnings and info\n",
    "import logging\n",
    "import warnings\n",
    "from langchain._api import LangChainDeprecationWarning\n",
    "warnings.simplefilter(\"ignore\", category=LangChainDeprecationWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "# import the necessary packages\n",
    "import uuid\n",
    "import sys, os\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from langchain import hub\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain.chains import RetrievalQA\n",
    "from nemoguardrails import RailsConfig, LLMRails\n",
    "\n",
    "# PATHs\n",
    "DATA_PATH=\"/mnt/c/Users/beene/Downloads/papers/\"\n",
    "DB_PATH = \"./chroma\"\n",
    "\n",
    "# Load or make the client\n",
    "print(\"Loading client ... \\n\")\n",
    "client = chromadb.PersistentClient(path = DB_PATH)\n",
    "\n",
    "# Preload the LLM\n",
    "llm = Ollama(\n",
    "    model = \"llama3\",\n",
    "    base_url = \"http://localhost:11434\",\n",
    "    temperature = 0,\n",
    "    verbose = True,\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")\n",
    "\n",
    "def load_guardrails():\n",
    "    # Nemo Guardrails\n",
    "    config = RailsConfig.from_path(\"./config/config.yml\")\n",
    "    rails = LLMRails(config, llm = llm)\n",
    "    return rails\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout    \n",
    "\n",
    "# Create a ChatPromptTemplate \n",
    "def create_prompt_template():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a research assistant specializing in question-answering tasks. \"\n",
    "            \"Use the provided context to answer the question concisely. \"\n",
    "            \"If you don't know the answer, simply state that you don't know. \"\n",
    "            \"Keep your response to a maximum of three sentences. \"\n",
    "            \"Keep following 3 instructions in the structured format. \"\n",
    "            \"1. Summarize the main points in bullet points, using •. This is the highest priority. \"\n",
    "            \"2. Provide a detailed description with sufficient detail for understanding, without unnecessary elaboration. \"\n",
    "            \"3. Include any additional relevant information, if available, but keep it brief and optional. \"\n",
    "        ),\n",
    "        (\n",
    "            \"user\", \n",
    "            \"Question: {question}\\n\" \n",
    "            \"Context: {context}\\n\"\n",
    "            \"Answer: \",\n",
    "        ),\n",
    "        (\n",
    "            \"assistant\", \n",
    "            \"**Main Points:**\\n\"\n",
    "            \"**Detailed Description:**\\n\"\n",
    "            \"**Additional Information:**\\n\"\n",
    "        )\n",
    "    ])\n",
    "    return prompt\n",
    "\n",
    "def count_pdfs():\n",
    "    import os\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(DATA_PATH):\n",
    "        count += len([fn for fn in files if fn.endswith(\".pdf\")])\n",
    "    return count\n",
    "\n",
    "# Vector DB functions\n",
    "def create_documents():\n",
    "    '''Upload the PDFs in the DATA_PATH directory'''\n",
    "    loader = PyPDFDirectoryLoader(DATA_PATH, silent_errors = False)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    return texts\n",
    "\n",
    "def create_embeddings(documents):\n",
    "    '''Create embeddings for the documents in the DATA_PATH directory'''\n",
    "    embedder = GPT4AllEmbeddings()\n",
    "    return [embedder.embed_query(doc.page_content) for doc in documents]\n",
    "\n",
    "def clear_db():\n",
    "    list_collections = client.list_collections()\n",
    "    if len(list_collections):\n",
    "        for i in list_collections:\n",
    "            client.delete_collection(i.dict()[\"name\"])\n",
    "\n",
    "def create_vector_db():\n",
    "    '''Create a vector db from the documents in the DATA_PATH directory'''\n",
    "    # Make original paper db and duplicated db for search purposes\n",
    "    # , which are saved to the DB_PATH directory\n",
    "    _ = client.create_collection(\n",
    "        name = \"Original\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "    _ = client.create_collection(\n",
    "        name = \"Search\",\n",
    "    )\n",
    "\n",
    "def add_documents_to_db():\n",
    "    # Load the vector db client\n",
    "    collection = client.get_collection(\"Original\")\n",
    "\n",
    "    # Create documents and embeddings\n",
    "    documents = create_documents()\n",
    "    embeddings = create_embeddings(documents)\n",
    "\n",
    "    # Add the documents and embeddings to the collection\n",
    "    collection.add(\n",
    "        ids = [str(uuid.uuid4()) for _ in documents],\n",
    "        documents = [doc.page_content for doc in documents],\n",
    "        metadatas = [doc.metadata for doc in documents],\n",
    "        embeddings = embeddings\n",
    "    )\n",
    "\n",
    "def implementation_db():\n",
    "    '''Implementation of the above functions'''\n",
    "    # Load LM    \n",
    "    print(\n",
    "        \"\\nCreate vector database ...\"\n",
    "        \"It could take a while depending on your dataset.\"\n",
    "    )\n",
    "    with HiddenPrints():\n",
    "        clear_db() # Clean up the existing db\n",
    "        create_vector_db()\n",
    "        add_documents_to_db()\n",
    "    print(\"Done.\\n\\n\")\n",
    "\n",
    "def load_vector_db(collection_name):\n",
    "    '''Load the vector db for chaining purposes'''\n",
    "    return Chroma(\n",
    "        client = client,\n",
    "        persist_directory = DB_PATH,\n",
    "        collection_name = collection_name,\n",
    "        embedding_function = GPT4AllEmbeddings(),\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "\n",
    "def extract_source_document(response):\n",
    "    '''Extract the source document from the response'''\n",
    "    source_document = response.get(\"source_documents\")[0].dict()\n",
    "    return source_document[\"metadata\"][\"source\"]\n",
    "\n",
    "def delete_searched_document(response):\n",
    "    '''Delete the searched document from the db'''\n",
    "    collection = client.get_collection(\"Search\")\n",
    "    collections = collection.get() # get a dict db\n",
    "\n",
    "    # find the id\n",
    "    source_document = extract_source_document(response)\n",
    "    ids = collections[\"ids\"]\n",
    "    ids_to_delete = []\n",
    "    for i in range(len(ids)):\n",
    "        if collections[\"metadatas\"][i][\"source\"] == source_document:\n",
    "            ids_to_delete.append(ids[i])\n",
    "\n",
    "    collection.delete(ids = ids_to_delete)\n",
    "\n",
    "\n",
    "# Retrieval QA functions\n",
    "# Contextual compression retriever\n",
    "def get_retrieval():\n",
    "    compressor = FlashrankRerank(top_n = 1)\n",
    "    chroma_db = load_vector_db(\"Search\")\n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor = compressor, \n",
    "        base_retriever = chroma_db.as_retriever()\n",
    "    )\n",
    "\n",
    "def get_chain(rail_llm):\n",
    "    guardrails = load_guardrails()\n",
    "    prompt = create_prompt_template()\n",
    "    retriever = get_retrieval()\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        rail_llm,\n",
    "        retriever = retriever,\n",
    "        chain_type_kwargs = {\"prompt\": prompt},\n",
    "        return_source_documents = True,\n",
    "    )\n",
    "\n",
    "def check_relevance_score(retriever, query):\n",
    "    '''Check the relevance score of the query'''\n",
    "    threshold = 0.5\n",
    "    retrieved_documents = retriever.invoke(query)\n",
    "    if len(retrieved_documents):\n",
    "        return retrieved_documents[0].metadata[\"relevance_score\"] >= threshold\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Make the response pretty\n",
    "def remove_empty_lines(result):\n",
    "    copy_result = result.copy()\n",
    "    for i in result:\n",
    "        if len(i) == 0:\n",
    "            copy_result.remove(i)\n",
    "    indices = []\n",
    "    for i in range(len(copy_result)):\n",
    "        if copy_result[i].endswith(\"**\") and i != 0:\n",
    "            indices.append(i)\n",
    "    for i in indices[::-1]:\n",
    "        copy_result.insert(i, \"\")         \n",
    "    return copy_result\n",
    "\n",
    "def print_bunch():\n",
    "    return print(\n",
    "        \"---------------------------------------\"\n",
    "        \"---------------------------------------\"\n",
    "        \"---------------------------------------\"\n",
    "    )\n",
    "\n",
    "def pprint_response(responses, response):\n",
    "    result = remove_empty_lines(response[\"result\"].split(\"\\n\")[1:])\n",
    "    file_path = response[\"source_documents\"][0].metadata[\"source\"]\n",
    "    file_name = Path(file_path).name\n",
    "    print_bunch()\n",
    "    print(f\"\\nDocuments #{len(responses)}\")\n",
    "    print(f\"The source document: {file_name}\\n\\n\")\n",
    "    _ = [print(phrase) for phrase in result]\n",
    "    print_bunch()\n",
    "\n",
    "# Main functions\n",
    "def ask_user_to_create_db():\n",
    "    while True:\n",
    "        user_input = input(\"Do you want to create a new database? (y/n): \")\n",
    "        if user_input == \"n\":\n",
    "            break\n",
    "        elif user_input == \"y\":\n",
    "            implementation_db()\n",
    "            break\n",
    "        print(\"Invalid input. Please enter \\'y\\' or \\'n\\'.\")\n",
    "\n",
    "def ask_how_many_documents():\n",
    "    # print(f\"\\n***Total documents: {count_pdfs()}***\")\n",
    "    while True:\n",
    "        iter_num = input(\n",
    "            f\"How many documents do you want to search? (<= {count_pdfs()}): \" \n",
    "        )\n",
    "        if iter_num.isdigit() and int(iter_num) <= count_pdfs():\n",
    "            return int(iter_num)\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "def prepare_db():\n",
    "    '''Duplicate the existing vector db for iterative search purposes'''\n",
    "    print(\"\\nPreparing a database for a search purpose ... \")\n",
    "    # Clear the search db\n",
    "    client.delete_collection(\"Search\")\n",
    "\n",
    "    # Load the vector db client\n",
    "    collection = client.get_collection(\"Original\")\n",
    "    collection2 = client.create_collection(\n",
    "        name = \"Search\", \n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "    \n",
    "    # Dupliacte a collection\n",
    "    total_documents_count = collection.count()\n",
    "    batch_size = 10\n",
    "    for i in range(0, total_documents_count, batch_size):\n",
    "        batch = collection.get(\n",
    "            include=[\"metadatas\", \"documents\", \"embeddings\"],\n",
    "            limit=batch_size,\n",
    "            offset=i\n",
    "        )\n",
    "        collection2.add(\n",
    "            ids=batch[\"ids\"],\n",
    "            documents=batch[\"documents\"],\n",
    "            metadatas=batch[\"metadatas\"],\n",
    "            embeddings=batch[\"embeddings\"]\n",
    "        )\n",
    "    print(\"Done.\\n\")\n",
    "\n",
    "def response_loop(iter_num):\n",
    "    assert iter_num <= count_pdfs()\n",
    "    '''Iterative search for documents'''\n",
    "    # params\n",
    "    rails = load_guardrails()\n",
    "    retriever = get_retrieval()\n",
    "    chain = get_chain(rails.llm)\n",
    "    \n",
    "    # Implement the search loop\n",
    "    responses = []\n",
    "    query = input(\"\\nWhat is your question?: \")\n",
    "    for _ in range(iter_num):\n",
    "        # Check if the search db is empty\n",
    "        if client.get_collection(\"Search\").count() == 0:\n",
    "            print(\"\\n=======================================\")\n",
    "            print(\"No documents left to search.\")\n",
    "            print(\"=======================================\\n\")\n",
    "            break\n",
    "\n",
    "        # Check the relevance score of the query\n",
    "        if check_relevance_score(retriever, query):\n",
    "            # Get response\n",
    "            with HiddenPrints():\n",
    "                response = chain.invoke(query)\n",
    "            \n",
    "            # Print the response (Custom)\n",
    "            responses.append(response)\n",
    "            pprint_response(responses, response)\n",
    "\n",
    "            # Delete the returned document from the db\n",
    "            delete_searched_document(response)\n",
    "        else: \n",
    "            # Quit the search loop if the relevance score is below the threshold\n",
    "            print(\"\\n=======================================\")\n",
    "            print(\"A searched document is not relevant.\")\n",
    "            print(\"=======================================\\n\")\n",
    "            break\n",
    "    print(\"\\n==================================\")    \n",
    "    print(f\"Total number of search is {len(responses)}.\")\n",
    "    print(\"Searching is done.\")\n",
    "    print(\"==================================\\n\")\n",
    "    return responses\n",
    "\n",
    "def ask_if_more_to_ask():\n",
    "    while True:\n",
    "        user_input = input(\"Do you want to ask more questions? (y/n): \")\n",
    "        if user_input == \"n\":\n",
    "            print(\"\\n\\nGoodbye!\")\n",
    "            quit()\n",
    "        elif user_input == \"y\":\n",
    "            return\n",
    "        print(\"Invalid input. Please enter \\'y\\' or \\'n\\'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "    rails = load_guardrails()\n",
    "    retriever = get_retrieval()\n",
    "    chain = get_chain(rails.llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the answer:\n",
      "\n",
      "**Main Points:**\n",
      "\n",
      "• EEG characteristics of conflict cognition are associated with increased alpha and theta power, and decreased beta power\n",
      "• Alpha and theta oscillations may reflect cognitive conflict and emotional arousal, while beta oscillations may indicate executive control and attentional resources\n",
      "\n",
      "**Detailed Description:**\n",
      "According to [24] Knyazev's research on motivation, emotion, and inhibitory control, EEG characteristics of conflict cognition are characterized by increased alpha (8-12 Hz) and theta (4-8 Hz) power, and decreased beta (13-30 Hz) power. Alpha and theta oscillations may reflect cognitive conflict and emotional arousal, while beta oscillations may indicate executive control and attentional resources.\n",
      "\n",
      "**Additional Information:**\n",
      "For a more comprehensive understanding of brain rhythms, I recommend [25] Buzsaki's book \"Rhythms of the Brain\", which provides an in-depth overview of neural oscillations and their roles in various cognitive processes."
     ]
    }
   ],
   "source": [
    "test = chain.invoke(\"What is EEG characteristics of conflict cognition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is EEG characteristics of conflict cognition?',\n",
       " 'result': 'Here is the answer:\\n\\n**Main Points:**\\n\\n• EEG characteristics of conflict cognition are associated with increased alpha and theta power, and decreased beta power\\n• Alpha and theta oscillations may reflect cognitive conflict and emotional arousal, while beta oscillations may indicate executive control and attentional resources\\n\\n**Detailed Description:**\\nAccording to [24] Knyazev\\'s research on motivation, emotion, and inhibitory control, EEG characteristics of conflict cognition are characterized by increased alpha (8-12 Hz) and theta (4-8 Hz) power, and decreased beta (13-30 Hz) power. Alpha and theta oscillations may reflect cognitive conflict and emotional arousal, while beta oscillations may indicate executive control and attentional resources.\\n\\n**Additional Information:**\\nFor a more comprehensive understanding of brain rhythms, I recommend [25] Buzsaki\\'s book \"Rhythms of the Brain\", which provides an in-depth overview of neural oscillations and their roles in various cognitive processes.',\n",
       " 'source_documents': [Document(page_content='[23] Tolegenova, A. A., Kustubayeva, A. M., & Matthews, G. (2014). \\nTrait meta-mood, gender and EEG response during emotion-regulation. Personality and Individual Differences, 65: \\n75-80. \\n[24] Knyazev, G. G. (2007). Motivation, emotion, and their inhibitory \\ncontrol mirrored in brain oscillations. Neuroscience & \\nBiobehavioral Reviews, 31(3): 377-395.  \\n[25] Buzsaki, G. (2006). Rhythms of the Brain. Oxford University Press. \\n \\nAuthorized licensed use limited to: Korea University. Downloaded on June 11,2024 at 13:43:55 UTC from IEEE Xplore.  Restrictions apply.', metadata={'page': 4, 'source': '/mnt/c/Users/beene/Downloads/papers/Gender_recognition_in_emotion_perception_using_EEG_features.pdf', 'relevance_score': 0.99617326})]}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nemo Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import RailsConfig\n",
    "from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails\n",
    "# Nemo Guardrails\n",
    "# ... initialize `some_chain`\n",
    "config = RailsConfig.from_path(\"./config/config.yml\")\n",
    "\n",
    "# Using LCEL, you first create a RunnableRails instance, and \"apply\" it using the \"|\" operator\n",
    "guardrails = RunnableRails(config)\n",
    "# chain_with_guardrails = guardrails | some_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db implementation\n",
    "if __name__==\"__main__\":\n",
    "    llm = load_llm()\n",
    "    client = chromadb.PersistentClient(path = DB_PATH)\n",
    "    # implementation_db(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make db for search and create chain\n",
    "duplicate_db(client)\n",
    "retriever = get_retrieval()\n",
    "chain = retrieval_qa_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = np.round(count_pdfs()*0.1).astype(int)\n",
    "query = \"What kind of experiments are implemented where PLV method is used?\"\n",
    "responses = response_loop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's my response:\n",
      "\n",
      "**Main Points:**\n",
      "\n",
      "* PLV (Phase-Locked Value) is calculated using instantaneous phase angles obtained by applying the Hilbert transformation to bandpass-filtered data.\n",
      "* The formula for calculating PLV between two signals A and B is: PLV = 1/T ∑t=1 e^(-i(φA(t) - φB(t)))\n",
      "* Three temporal features are extracted from each PLV time series: mean, variability (standard deviation), and sample entropy.\n",
      "\n",
      "**Detailed Description:**\n",
      "\n",
      "To calculate the PLV, instantaneous phase angles are obtained by applying the Hilbert transformation to the bandpass-filtered data. The formula for calculating PLV between two signals A and B is then used, where ϕA(t) and ϕB(t) are the instantaneous phase angles of each EEG signal.\n",
      "\n",
      "**Additional Information:**\n",
      "\n",
      "Sample entropy is a non-linear measure that quantifies the degree of complexity in a time series (Richman and Moorman, 2000)."
     ]
    }
   ],
   "source": [
    "test = chain.invoke(\"How can PLV be calculated?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent-Xw65Epl3-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
