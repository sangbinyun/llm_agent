{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from scholarly import scholarly\n",
    "from langchain_community.tools.google_scholar import GoogleScholarQueryRun\n",
    "from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nemo Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import RailsConfig\n",
    "from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails\n",
    "# Nemo Guardrails\n",
    "# ... initialize `some_chain`\n",
    "config = RailsConfig.from_path(\"path/to/config\")\n",
    "\n",
    "# Using LCEL, you first create a RunnableRails instance, and \"apply\" it using the \"|\" operator\n",
    "guardrails = RunnableRails(config)\n",
    "chain_with_guardrails = guardrails | some_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langserve import add_routes\n",
    "\n",
    "# 1. Create prompt template\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('user', '{text}')\n",
    "])\n",
    "\n",
    "# 2. Create model\n",
    "# model = ChatOpenAI()\n",
    "\n",
    "# 3. Create parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4. Create chain\n",
    "chain = prompt_template | llm | parser\n",
    "\n",
    "\n",
    "# 4. App definition\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 5. Adding chain route\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/chain\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector db #1\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader, BSHTMLLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings  \n",
    "\n",
    "# DATA_PATH=\"/mnt/c/Users/beene/Downloads/papers/\"\n",
    "DATA_PATH=\"/mnt/c/Users/beene/Downloads/tests/\"\n",
    "DB_PATH = \"./vectorstores/db/\"\n",
    "\n",
    "#load the LLM\n",
    "def load_llm():\n",
    "    llm = Ollama(\n",
    "        model=\"llama3\",\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        verbose=True,\n",
    "        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def create_vector_db():\n",
    "    loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    documents = loader.load()\n",
    "    print(f\"Processed {len(documents)} pdf files\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    texts=text_splitter.split_documents(documents)\n",
    "    vectorstore = Chroma.from_documents(documents=texts, embedding=OllamaEmbeddings(), persist_directory=DB_PATH)\n",
    "\n",
    "def load_vector_db():\n",
    "    return Chroma(persist_directory=DB_PATH, embedding_function=OllamaEmbeddings())\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    llm = load_llm()\n",
    "    # create_vector_db()\n",
    "    vectorstore = load_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "from langserve import add_routes\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"Spin up a simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    llm,\n",
    "    path=\"/llm\",\n",
    ")\n",
    "# add_routes(\n",
    "#     app,\n",
    "#     ChatAnthropic(model=\"claude-3-haiku-20240307\"),\n",
    "#     path=\"/anthropic\",\n",
    "# )\n",
    "\n",
    "async def main():\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector db #2\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings  \n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.create_collection(\"default\")\n",
    "collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "# langchain_chroma = Chroma(\n",
    "#     client=persistent_client,\n",
    "#     collection_name=\"default\",\n",
    "#     embedding_function=OllamaEmbeddings(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chroma.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = chromadb.PersistentClient(\n",
    "    path = \"./vectorstores/db/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = Chroma(\n",
    "    client = test,\n",
    "    collection_name = \"default\",\n",
    "    embedding_function = GPT4AllEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual compression retriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "\n",
    "compressor = FlashrankRerank(top_n = 1)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")\n",
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multiquery_retriever = MultiQueryRetriever.from_llm( \n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    ")\n",
    "compression_retriever.invoke(\"What did the president say about Ketanji Jackson Brown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required dependencies\n",
    "from langchain import hub\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import chainlit as cl\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "# Set up RetrievelQA model\n",
    "QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt-mistral\")\n",
    "\n",
    "\n",
    "def retrieval_qa_chain(llm,vectorstore):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "def qa_bot(): \n",
    "    llm=load_llm() \n",
    "    vectorstore = load_vector_db()\n",
    "\n",
    "    qa = retrieval_qa_chain(llm,vectorstore)\n",
    "    return qa \n",
    "\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    chain=qa_bot()\n",
    "    msg=cl.Message(content=\"Firing up the research info bot...\")\n",
    "    await msg.send()\n",
    "    msg.content= \"Hi, welcome to research info bot. What is your query?\"\n",
    "    await msg.update()\n",
    "    cl.user_session.set(\"chain\",chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message):\n",
    "    chain=cl.user_session.get(\"chain\")\n",
    "    cb = cl.AsyncLangchainCallbackHandler(\n",
    "    stream_final_answer=True,\n",
    "    answer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\n",
    "    )\n",
    "    cb.answer_reached=True\n",
    "    # res=await chain.acall(message, callbacks=[cb])\n",
    "    res=await chain.acall(message.content, callbacks=[cb])\n",
    "    print(f\"response: {res}\")\n",
    "    answer=res[\"result\"]\n",
    "    answer=answer.replace(\".\",\".\\n\")\n",
    "    sources=res[\"source_documents\"]\n",
    "\n",
    "    if sources:\n",
    "        answer+=f\"\\nSources: \"+str(str(sources))\n",
    "    else:\n",
    "        answer+=f\"\\nNo Sources found\"\n",
    "\n",
    "    await cl.Message(content=answer).send() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "#load the LLM\n",
    "def load_llm():\n",
    "    llm = Ollama(\n",
    "        model = \"llama3\",\n",
    "        base_url = \"http://localhost:11434\",\n",
    "        temperature = 0,\n",
    "        verbose = True,\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Set up RetrievelQA model\n",
    "prompt = hub.pull(\"rlm/rag-prompt-llama3\")\n",
    "\n",
    "# Create a ChatPromptTemplate same with the above llama3 prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a research assistant specializing in question-answering tasks. \"\n",
    "        \"Use the provided context to answer the question concisely. \"\n",
    "        \"If you don't know the answer, simply state that you don't know. \"\n",
    "        \"Keep your response to a maximum of three sentences. \"\n",
    "        \"Summarize the main points in bullet points. This is the highest priority. \"\n",
    "        \"Provide a detailed description with sufficient detail for understanding, without unnecessary elaboration. \"\n",
    "        \"Include any additional relevant information, if available, but keep it brief and optional. \"\n",
    "    ),\n",
    "    (\n",
    "        \"user\", \n",
    "        \"Question: {question}\\n\" \n",
    "        \"Context: {context}\\n\"\n",
    "        # \"Requests:\\n\"\n",
    "        # \"0. Start your response directly without the first introductory phrases.\\n\"\n",
    "        # \"0. Start your response with a cute and unique arrow emoji.\\n\"\n",
    "        # \"1. Summarize the main points in bullet points. This is the highest priority.\\n\"\n",
    "        # \"2. Provide a detailed description with sufficient detail for understanding, without unnecessary elaboration.\\n\"\n",
    "        # \"3. Include any additional relevant information, if available, but keep it brief and optional.\\n\"\n",
    "        \"Answer:\",\n",
    "    ),\n",
    "    (\n",
    "        \"assistant\", \n",
    "        \"**Main Points:**\\n\"\n",
    "        \"**Detailed Description:**\\n\"\n",
    "        \"**Additional Information:**\\n\"\n",
    "    )\n",
    "])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "# from gpt4all import GPT4AllEmbedding\n",
    "\n",
    "# DATA_PATH=\"/mnt/c/Users/beene/Downloads/papers/\"\n",
    "DATA_PATH=\"/mnt/c/Users/beene/Downloads/tests/\"\n",
    "DB_PATH = \"./chroma\"\n",
    "\n",
    "def create_documents():\n",
    "    loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    return texts\n",
    "\n",
    "def create_embeddings(documents):\n",
    "    '''Create embeddings for the documents in the DATA_PATH directory'''\n",
    "    embedder = GPT4AllEmbeddings()\n",
    "    return [embedder.embed_query(doc.page_content) for doc in documents]\n",
    "\n",
    "def clear_db(client):\n",
    "    list_collections = client.list_collections()\n",
    "    if len(list_collections):\n",
    "        for i in list_collections:\n",
    "            client.delete_collection(i.dict()[\"name\"])\n",
    "\n",
    "def create_vector_db(client):\n",
    "    '''Create a vector db from the documents in the DATA_PATH directory'''\n",
    "    # Make original paper db and duplicated db for search purposes\n",
    "    # , which are saved to the DB_PATH directory\n",
    "    _ = client.create_collection(\n",
    "        name = \"Original\",\n",
    "    )\n",
    "    _ = client.create_collection(\n",
    "        name = \"Search\",\n",
    "    )\n",
    "\n",
    "def add_documents_to_db(client):\n",
    "    # Load the vector db client\n",
    "    collection = client.get_collection(\"Original\")\n",
    "\n",
    "    # Create documents and embeddings\n",
    "    documents = create_documents()\n",
    "    embeddings = create_embeddings(documents)\n",
    "\n",
    "    # Add the documents and embeddings to the collection\n",
    "    collection.add(\n",
    "        ids = [str(uuid.uuid4()) for _ in documents],\n",
    "        documents = [doc.page_content for doc in documents],\n",
    "        metadatas = [doc.metadata for doc in documents],\n",
    "        embeddings = embeddings\n",
    "    )\n",
    "\n",
    "def duplicate_db(client):\n",
    "    '''Duplicate the existing vector db for iterative search purposes'''\n",
    "    # Clear the search db\n",
    "    client.delete_collection(\"Search\")\n",
    "\n",
    "    # Load the vector db client\n",
    "    collection = client.get_collection(\"Original\")\n",
    "    collection2 = client.create_collection(\"Search\")\n",
    "    \n",
    "    # Dupliacte a collection\n",
    "    total_documents_count = collection.count()\n",
    "    batch_size = 10\n",
    "    for i in range(0, total_documents_count, batch_size):\n",
    "        batch = collection.get(\n",
    "            include=[\"metadatas\", \"documents\", \"embeddings\"],\n",
    "            limit=batch_size,\n",
    "            offset=i\n",
    "        )\n",
    "        collection2.add(\n",
    "            ids=batch[\"ids\"],\n",
    "            documents=batch[\"documents\"],\n",
    "            metadatas=batch[\"metadatas\"],\n",
    "            embeddings=batch[\"embeddings\"]\n",
    "        )    \n",
    "\n",
    "def implementation_db(client):\n",
    "    '''Implementation of the above functions'''\n",
    "    clear_db(client) # Clean up the existing db\n",
    "    create_vector_db(client)\n",
    "    add_documents_to_db(client)    \n",
    "\n",
    "def load_vector_db(collection_name):\n",
    "    '''Load the vector db for chaining purposes'''\n",
    "    return Chroma(\n",
    "        persist_directory = DB_PATH,\n",
    "        collection_name = collection_name,\n",
    "        embedding_function = GPT4AllEmbeddings()\n",
    "    )\n",
    "\n",
    "def extract_source_document(response):\n",
    "    '''Extract the source document from the response'''\n",
    "    source_document = response.get(\"source_documents\")[0].dict()\n",
    "    return source_document[\"metadata\"][\"source\"]\n",
    "\n",
    "def delete_searched_document(client, response):\n",
    "    '''Delete the searched document from the db'''\n",
    "    collection = client.get_collection(\"Search\")\n",
    "    collections = collection.get() # get a dict db\n",
    "\n",
    "    # find the id\n",
    "    source_document = extract_source_document(response)\n",
    "    ids = collections[\"ids\"]\n",
    "    ids_to_delete = []\n",
    "    for i in range(len(ids)):\n",
    "        if collections[\"metadatas\"][i][\"source\"] == source_document:\n",
    "            ids_to_delete.append(ids[i])\n",
    "\n",
    "    collection.delete(ids = ids_to_delete)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    llm = load_llm()\n",
    "    client = chromadb.PersistentClient(path = DB_PATH)\n",
    "    implementation_db(client)\n",
    "    duplicate_db(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_searched_document(client, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual compression retriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "compressor = FlashrankRerank(top_n = 1)\n",
    "chroma_db = load_vector_db(\"Search\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=chroma_db.as_retriever()\n",
    ")\n",
    "\n",
    "def retrieval_qa_chain(llm,vectorstore):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=compression_retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.api.segment:Collection ResearchPapers is not created.\n"
     ]
    }
   ],
   "source": [
    "def qa_bot(): \n",
    "    llm=load_llm() \n",
    "    chroma_db = load_vector_db(\"ResearchPapers\")\n",
    "\n",
    "    qa = retrieval_qa_chain(llm, chroma_db)\n",
    "    return qa \n",
    "\n",
    "\n",
    "\n",
    "chain=qa_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Based on the provided context, I found that PLV stands for Phase-Locked Value.\n",
      "\n",
      "**Main Points:**\n",
      "\n",
      "* PLV is an abbreviation for Phase-Locked Value.\n",
      "* It is a measure used in neuroscience to quantify the synchronization of neural activity between different brain regions.\n",
      "\n",
      "**Detailed Description:**\n",
      "PLV is a statistical method used to analyze the phase-locking behavior of neural oscillations. It measures the degree to which two or more brain regions are synchronized in terms of their neural activity, particularly in the frequency domain.\n",
      "\n",
      "**Additional Information:**\n",
      "None available in this context."
     ]
    }
   ],
   "source": [
    "test = chain.invoke(\"What is PLV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/beene/Downloads/tests/Schizophrenia ref1.pdf'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.get(\"source_documents\")[0].dict()[\"metadata\"][\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "for i in range(len(test[\"ids\"])):\n",
    "    test_list.append(test[\"documents\"][i]) if test[\"metadatas\"][i][\"page\"] == 0 else None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent-Xw65Epl3-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
